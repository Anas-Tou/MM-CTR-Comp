{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13839090,"sourceType":"datasetVersion","datasetId":8814081}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WWW 2025: CLIP-Enhanced Multimodal CTR Solution\n\nThis notebook implements a two-stage advanced solution:\n\n### **Stage 1: CLIP Embedding Generation (Task 1)**\n* **Model:** `openai/clip-vit-base-patch32` (Pre-trained Vision Transformer).\n* **Hardware:** Utilizes **Dual T4 GPUs** for parallel inference.\n* **Process:** Reads raw images from `/kaggle/input/microlens/item_images`, encodes them into rich semantic vectors, and projects them to 128-d using PCA.\n\n### **Stage 2: Transformer_DCN Training (Task 2)**\n* **Model:** Team momo's **Transformer_DCN** architecture.\n* **Input:** The **NEW** CLIP-generated embeddings + User ID sequences.\n* **Goal:** High-performance CTR prediction.","metadata":{}},{"cell_type":"code","source":"# 1. Setup Environment\n!pip install fuxictr==2.3.7 pandas==2.2.3 scikit-learn==1.4.0 transformers==4.38.0 pillow -q\n\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.decomposition import PCA\n\n# Check GPUs\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n    print(f\"Devices: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:44.848232Z","iopub.execute_input":"2025-12-05T15:26:44.849008Z","iopub.status.idle":"2025-12-05T15:27:10.373894Z","shell.execute_reply.started":"2025-12-05T15:26:44.848977Z","shell.execute_reply":"2025-12-05T15:27:10.373172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stage 1: Generate CLIP Embeddings","metadata":{}},{"cell_type":"code","source":"# 2. Define Image Dataset & CLIP Extractor\n\nclass RawImageDataset(Dataset):\n    def __init__(self, item_ids, img_root, processor):\n        self.item_ids = item_ids\n        self.img_root = img_root\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.item_ids)\n\n    def __getitem__(self, idx):\n        item_id = self.item_ids[idx]\n        # Handle potential jpg/png extensions\n        img_path = os.path.join(self.img_root, f\"{item_id}.jpg\")\n        if not os.path.exists(img_path):\n            img_path = os.path.join(self.img_root, f\"{item_id}.png\")\n        \n        try:\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert(\"RGB\")\n            else:\n                # Fallback black image if missing\n                image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n            \n            # Process image for CLIP\n            inputs = self.processor(images=image, return_tensors=\"pt\")\n            return inputs[\"pixel_values\"].squeeze(0)  # [3, 224, 224]\n        except Exception as e:\n            print(f\"Error loading {item_id}: {e}\")\n            return torch.zeros(3, 224, 224)\n\ndef generate_clip_embeddings():\n    # Configuration\n    IMG_ROOT = \"/kaggle/input/microlens/item_images/item_images\"\n    ITEM_INFO_PATH = \"/kaggle/input/microlens/MicroLens_1M_x1/item_info.parquet\"\n    BATCH_SIZE = 256  # Adjust for GPU memory\n    MODEL_ID = \"openai/clip-vit-base-patch32\"\n    \n    # 1. Load Item IDs\n    if not os.path.exists(ITEM_INFO_PATH):\n        # Fallback search if path varies\n        import glob\n        found = glob.glob(\"/kaggle/input/**/item_info.parquet\", recursive=True)\n        if found: ITEM_INFO_PATH = found[0]\n        else: \n            print(\"Dataset not found!\")\n            return None\n\n    print(f\"Loading metadata from {ITEM_INFO_PATH}...\")\n    df_items = pd.read_parquet(ITEM_INFO_PATH)\n    item_ids = df_items['item_id'].tolist()\n    \n    # 2. Setup Model (Dual GPU)\n    print(f\"Loading {MODEL_ID}...\")\n    processor = CLIPProcessor.from_pretrained(MODEL_ID)\n    model = CLIPModel.from_pretrained(MODEL_ID)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for inference!\")\n        model = torch.nn.DataParallel(model)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    # 3. Inference Loop\n    dataset = RawImageDataset(item_ids, IMG_ROOT, processor)\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    \n    all_embeddings = []\n    print(\"Extracting features...\")\n    with torch.no_grad():\n        for batch_imgs in tqdm(dataloader):\n            batch_imgs = batch_imgs.to(device)\n            # Get image features from CLIP\n            if isinstance(model, torch.nn.DataParallel):\n                features = model.module.get_image_features(pixel_values=batch_imgs)\n            else:\n                features = model.get_image_features(pixel_values=batch_imgs)\n            \n            all_embeddings.append(features.cpu().numpy())\n            \n    raw_embeddings = np.concatenate(all_embeddings, axis=0)\n    print(f\"Raw Embedding Shape: {raw_embeddings.shape}\")\n\n    # 4. Project to 128-dim using PCA (Task 1 Requirement)\n    print(\"Reducing dimensions to 128 (PCA)...\")\n    pca = PCA(n_components=128)\n    emb_128 = pca.fit_transform(raw_embeddings)\n    \n    # 5. Save New Dataset\n    os.makedirs(\"data/MicroLens_1M_x1\", exist_ok=True)\n    df_items['item_emb_d128'] = list(emb_128)\n    \n    output_path = \"data/MicroLens_1M_x1/item_info_task1.parquet\"\n    df_items.to_parquet(output_path)\n    print(f\"✓ New Item Info saved to: {output_path}\")\n    return output_path\n\n# Run the extraction\n# Note: If item_images folder is missing in this specific environment, \n# this block handles exceptions gracefully.\nif os.path.exists(\"/kaggle/input/microlens/item_images\"):\n    new_item_info_path = generate_clip_embeddings()\nelse:\n    print(\"⚠️ Image directory not found. Skipping CLIP generation for demo.\")\n    # Use existing if images missing (fallback)\n    new_item_info_path = \"./data/MicroLens_1M_x1/item_info.parquet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:41:36.692230Z","iopub.execute_input":"2025-12-05T15:41:36.692810Z","iopub.status.idle":"2025-12-05T15:52:26.314145Z","shell.execute_reply.started":"2025-12-05T15:41:36.692782Z","shell.execute_reply":"2025-12-05T15:52:26.313095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stage 2: Transformer_DCN Setup & Training","metadata":{}},{"cell_type":"code","source":"# 3. Create Model Code (FuxiCTR + Momo Architecture)\nimport os\n\n# --- FIX: Create directories first ---\nos.makedirs(\"src\", exist_ok=True)\nos.makedirs(\"config\", exist_ok=True)\nos.makedirs(\"config/Transformer_DCN_microlens_mmctr_tuner_config_01\", exist_ok=True)\nos.makedirs(\"config/transformer_dcn_config\", exist_ok=True)\n# -------------------------------------\n\nfiles = {}\n\nfiles[\"fuxictr_version.py\"] = \"\"\"import fuxictr\\nassert fuxictr.__version__ == \"2.3.7\"\\n\"\"\"\nfiles[\"src/__init__.py\"] = \"\"\"from .mmctr_dataloader import MMCTRDataLoader\\nfrom .DIN import DIN\\nfrom .Transformer_DCN import Transformer_DCN\\nfrom .Transformer_DCN_Quant import Transformer_DCN_Quant\"\"\"\n\nfiles[\"src/mmctr_dataloader.py\"] = \"\"\"import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nimport pandas as pd\nimport torch\n\nclass ParquetDataset(Dataset):\n    def __init__(self, data_path):\n        self.column_index = dict()\n        self.darray = self.load_data(data_path)\n    def __getitem__(self, index):\n        return self.darray[index, :]\n    def __len__(self):\n        return self.darray.shape[0]\n    def load_data(self, data_path):\n        df = pd.read_parquet(data_path)\n        data_arrays = []\n        idx = 0\n        for col in df.columns:\n            if df[col].dtype == \"object\":\n                array = np.array(df[col].to_list())\n                seq_len = array.shape[1]\n                self.column_index[col] = [i + idx for i in range(seq_len)]\n                idx += seq_len\n            else:\n                array = df[col].to_numpy()\n                self.column_index[col] = idx\n                idx += 1\n            data_arrays.append(array)\n        return np.column_stack(data_arrays)\n\nclass MMCTRDataLoader(DataLoader):\n    def __init__(self, feature_map, data_path, item_info, batch_size=32, shuffle=False,\n                 num_workers=1, max_len=100, **kwargs):\n        if not data_path.endswith(\".parquet\"):\n            data_path += \".parquet\"\n        self.dataset = ParquetDataset(data_path)\n        column_index = self.dataset.column_index\n        super().__init__(dataset=self.dataset, batch_size=batch_size,\n                         shuffle=shuffle, num_workers=num_workers,\n                         collate_fn=BatchCollator(feature_map, max_len, column_index, item_info))\n        self.num_samples = len(self.dataset)\n        self.num_blocks = 1\n        self.num_batches = int(np.ceil(self.num_samples / self.batch_size))\n    def __len__(self):\n        return self.num_batches\n\nclass BatchCollator(object):\n    def __init__(self, feature_map, max_len, column_index, item_info):\n        self.feature_map = feature_map\n        self.item_info = pd.read_parquet(item_info)\n        self.max_len = max_len\n        self.column_index = column_index\n    def __call__(self, batch):\n        batch_tensor = default_collate(batch)\n        all_cols = set(list(self.feature_map.features.keys()) + self.feature_map.labels)\n        batch_dict = dict()\n        for col, idx in self.column_index.items():\n            if col in all_cols:\n                batch_dict[col] = batch_tensor[:, idx]\n        batch_seqs = batch_dict[\"item_seq\"][:, -self.max_len:]\n        del batch_dict[\"item_seq\"]\n        mask = (batch_seqs > 0).float()\n        item_index = batch_dict[\"item_id\"].numpy().reshape(-1, 1)\n        del batch_dict[\"item_id\"]\n        batch_items = np.hstack([batch_seqs.numpy(), item_index]).flatten()\n        item_info = self.item_info.iloc[batch_items]\n        item_dict = dict()\n        for col in item_info.columns:\n            if col in all_cols:\n                item_dict[col] = torch.from_numpy(np.array(item_info[col].to_list()))\n        return batch_dict, item_dict, mask\"\"\"\n\nfiles[\"src/Transformer_DCN.py\"] = \"\"\"import torch\nfrom fuxictr.utils import not_in_whitelist\nfrom torch import nn\nfrom fuxictr.pytorch.models import BaseModel\nfrom fuxictr.pytorch.layers import FeatureEmbedding, MLP_Block, CrossNetV2\n\nclass Transformer_DCN(BaseModel):\n    def __init__(self, feature_map, model_id=\"Transformer_DCN\", gpu=-1, hidden_activations=\"ReLU\",\n                 dcn_cross_layers=3, dcn_hidden_units=[1024, 512, 256], mlp_hidden_units=[64, 32],\n                 num_heads=1, transformer_layers=2, transformer_dropout=0.2, dim_feedforward=256,\n                 learning_rate=5e-4, embedding_dim=64, net_dropout=0.2, first_k_cols=16,\n                 batch_norm=False, concat_max_pool=True, accumulation_steps=1,\n                 embedding_regularizer=None, net_regularizer=None, **kwargs):\n        super().__init__(feature_map, model_id=model_id, gpu=gpu, embedding_regularizer=embedding_regularizer,\n                         net_regularizer=net_regularizer, **kwargs)\n        self.feature_map = feature_map\n        self.embedding_dim = embedding_dim\n        self.item_info_dim = 0\n        for feat, spec in self.feature_map.features.items():\n            if spec.get(\"source\") == \"item\":\n                self.item_info_dim += spec.get(\"embedding_dim\", embedding_dim)\n        transformer_in_dim = self.item_info_dim * 2\n        self.accumulation_steps = accumulation_steps\n        self.embedding_layer = FeatureEmbedding(feature_map, embedding_dim)\n        self.transformer_encoder = Transformer(transformer_in_dim, dim_feedforward=dim_feedforward,\n            num_heads=num_heads, dropout=transformer_dropout, transformer_layers=transformer_layers,\n            first_k_cols=first_k_cols, concat_max_pool=concat_max_pool)\n        seq_out_dim = (first_k_cols + int(concat_max_pool)) * transformer_in_dim\n        dcn_in_dim = feature_map.sum_emb_out_dim() + seq_out_dim\n        self.crossnet = CrossNetV2(dcn_in_dim, dcn_cross_layers)\n        self.parallel_dnn = MLP_Block(input_dim=dcn_in_dim, output_dim=None, hidden_units=dcn_hidden_units,\n                                      hidden_activations=hidden_activations, output_activation=None,\n                                      dropout_rates=net_dropout, batch_norm=batch_norm)\n        dcn_out_dim = dcn_in_dim + dcn_hidden_units[-1]\n        self.mlp = MLP_Block(input_dim=dcn_out_dim, output_dim=1, hidden_units=mlp_hidden_units,\n                             hidden_activations=hidden_activations, output_activation=self.output_activation)\n        self.compile(kwargs[\"optimizer\"], kwargs[\"loss\"], learning_rate)\n        self.reset_parameters()\n        self.model_to_device()\n    def forward(self, inputs):\n        batch_dict, item_dict, mask = self.get_inputs(inputs)\n        emb_list = []\n        if batch_dict:\n            feature_emb = self.embedding_layer(batch_dict, flatten_emb=True)\n            emb_list.append(feature_emb)\n        feat_emb = torch.cat(emb_list, dim=-1)\n        item_feat_emb = self.embedding_layer(item_dict, flatten_emb=True)\n        batch_size = mask.shape[0]\n        item_feat_emb = item_feat_emb.view(batch_size, -1, self.item_info_dim)\n        target_emb = item_feat_emb[:, -1, :]\n        sequence_emb = item_feat_emb[:, 0:-1, :]\n        transformer_emb = self.transformer_encoder(target_emb, sequence_emb, mask=mask)\n        dcn_in_emb = torch.cat([feat_emb, target_emb, transformer_emb], dim=-1)\n        cross_out = self.crossnet(dcn_in_emb)\n        dnn_out = self.parallel_dnn(dcn_in_emb)\n        y_pred = self.mlp(torch.cat([cross_out, dnn_out], dim=-1))\n        return {\"y_pred\": y_pred}\n    def get_inputs(self, inputs, feature_source=None):\n        batch_dict, item_dict, mask = inputs\n        X_dict = dict()\n        for feature, value in batch_dict.items():\n            if feature in self.feature_map.labels: continue\n            feature_spec = self.feature_map.features[feature]\n            if feature_spec[\"type\"] == \"meta\": continue\n            if feature_source and not_in_whitelist(feature_spec[\"source\"], feature_source): continue\n            X_dict[feature] = value.to(self.device)\n        for item, value in item_dict.items():\n            item_dict[item] = value.to(self.device)\n        return X_dict, item_dict, mask.to(self.device)\n    def concat_embedding(self, field, feature_emb_dict):\n        if type(field) == tuple:\n            emb_list = [feature_emb_dict[f] for f in field]\n            return torch.cat(emb_list, dim=-1)\n        else:\n            return feature_emb_dict[field]\n    def get_labels(self, inputs):\n        labels = self.feature_map.labels\n        batch_dict = inputs[0]\n        y = batch_dict[labels[0]].to(self.device)\n        return y.float().view(-1, 1)\n    def get_group_id(self, inputs):\n        return inputs[0][self.feature_map.group_id]\n    def train_step(self, batch_data):\n        return_dict = self.forward(batch_data)\n        y_true = self.get_labels(batch_data)\n        loss = self.compute_loss(return_dict, y_true)\n        loss = loss / self.accumulation_steps\n        loss.backward()\n        if (self._batch_index + 1) % self.accumulation_steps == 0:\n            nn.utils.clip_grad_norm_(self.parameters(), self._max_gradient_norm)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n        return loss\n\nclass Transformer(nn.Module):\n    def __init__(self, transformer_in_dim, dim_feedforward=64, num_heads=1, dropout=0,\n                 transformer_layers=1, first_k_cols=16, concat_max_pool=True):\n        super(Transformer, self).__init__()\n        self.concat_max_pool = concat_max_pool\n        self.first_k_cols = first_k_cols\n        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_in_dim, nhead=num_heads,\n            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n        if self.concat_max_pool:\n            self.out_linear = nn.Linear(transformer_in_dim, transformer_in_dim)\n    def forward(self, target_emb, sequence_emb, mask=None):\n        seq_len = sequence_emb.size(1)\n        concat_seq_emb = torch.cat([sequence_emb, target_emb.unsqueeze(1).expand(-1, seq_len, -1)], dim=-1)\n        key_padding_mask = self.adjust_mask(mask).bool()\n        tfmr_out = self.transformer_encoder(src=concat_seq_emb, src_key_padding_mask=key_padding_mask)\n        tfmr_out = tfmr_out.masked_fill(key_padding_mask.unsqueeze(-1).repeat(1, 1, tfmr_out.shape[-1]), 0.)\n        output_concat = []\n        output_concat.append(tfmr_out[:, -self.first_k_cols:].flatten(start_dim=1))\n        if self.concat_max_pool:\n            tfmr_out = tfmr_out.masked_fill(key_padding_mask.unsqueeze(-1).repeat(1, 1, tfmr_out.shape[-1]), -1e9)\n            pooled_out = self.out_linear(tfmr_out.max(dim=1).values)\n            output_concat.append(pooled_out)\n        return torch.cat(output_concat, dim=-1)\n    def adjust_mask(self, mask):\n        fully_masked = mask.all(dim=-1)\n        mask[fully_masked, -1] = 0\n        return mask\"\"\"\n\nfiles[\"run_expid.py\"] = \"\"\"import os\nimport sys\nimport logging\nimport fuxictr_version\nfrom fuxictr import datasets\nfrom datetime import datetime\nfrom fuxictr.utils import load_config, set_logger, print_to_json, print_to_list\nfrom fuxictr.features import FeatureMap\nfrom fuxictr.pytorch.dataloaders import RankDataLoader\nfrom fuxictr.pytorch.torch_utils import seed_everything\nfrom fuxictr.preprocess import FeatureProcessor, build_dataset\nimport src as model_zoo\nfrom src.mmctr_dataloader import MMCTRDataLoader\nimport gc\nimport argparse\nimport os\nfrom pathlib import Path\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default='./config/', help='The config directory.')\n    parser.add_argument('--expid', type=str, default='DeepFM_test', help='The experiment id to run.')\n    parser.add_argument('--gpu', type=int, default=-1, help='The gpu index, -1 for cpu')\n    args = vars(parser.parse_args())\n    experiment_id = args['expid']\n    params = load_config(args['config'], experiment_id)\n    params['gpu'] = args['gpu']\n    set_logger(params)\n    logging.info(\"Params: \" + print_to_json(params))\n    seed_everything(seed=params['seed'])\n    data_dir = os.path.join(params['data_root'], params['dataset_id'])\n    feature_map_json = os.path.join(data_dir, \"feature_map.json\")\n    feature_encoder = FeatureProcessor(**params)\n    params[\"train_data\"], params[\"valid_data\"], params[\"test_data\"] = build_dataset(feature_encoder, **params)\n    feature_map = FeatureMap(params['dataset_id'], data_dir)\n    feature_map.load(feature_map_json, params)\n    logging.info(\"Feature specs: \" + print_to_json(feature_map.features))\n    model_class = getattr(model_zoo, params['model'])\n    model = model_class(feature_map, **params)\n    model.count_parameters()\n    params[\"data_loader\"] = MMCTRDataLoader\n    train_gen, valid_gen = RankDataLoader(feature_map, stage='train', **params).make_iterator()\n    model.fit(train_gen, validation_data=valid_gen, **params)\n    logging.info('****** Validation evaluation ******')\n    valid_result = model.evaluate(valid_gen)\n    result_filename = Path(args['config']).name.replace(\".yaml\", \"\") + '.csv'\n    with open(result_filename, 'a+') as fw:\n        fw.write(' {},[command] python {},[exp_id] {},[dataset_id] {},[train] {},[val] {}\\\\n'.format(datetime.now().strftime('%Y%m%d-%H%M%S'), ' '.join(sys.argv), experiment_id, params['dataset_id'], \"N.A.\", print_to_list(valid_result)))\"\"\"\n\nfiles[\"prediction.py\"] = \"\"\"import os\nimport sys\nimport logging\nimport fuxictr_version\nfrom fuxictr import datasets\nfrom datetime import datetime\nfrom fuxictr.utils import load_config, set_logger, print_to_json, print_to_list\nfrom fuxictr.features import FeatureMap\nfrom fuxictr.pytorch.dataloaders import RankDataLoader\nfrom fuxictr.pytorch.torch_utils import seed_everything\nfrom fuxictr.preprocess import FeatureProcessor, build_dataset\nimport src as model_zoo\nfrom src.mmctr_dataloader import MMCTRDataLoader\nimport gc\nimport argparse\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport shutil\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default='./config/', help='The config directory.')\n    parser.add_argument('--expid', type=str, default='DeepFM_test', help='The experiment id to run.')\n    parser.add_argument('--gpu', type=int, default=-1, help='The gpu index, -1 for cpu')\n    args = vars(parser.parse_args())\n    experiment_id = args['expid']\n    params = load_config(args['config'], experiment_id)\n    params['gpu'] = args['gpu']\n    set_logger(params)\n    logging.info(\"Params: \" + print_to_json(params))\n    seed_everything(seed=params['seed'])\n    data_dir = os.path.join(params['data_root'], params['dataset_id'])\n    feature_map_json = os.path.join(data_dir, \"feature_map.json\")\n    feature_encoder = FeatureProcessor(**params)\n    params[\"train_data\"], params[\"valid_data\"], params[\"test_data\"] = build_dataset(feature_encoder, **params)\n    feature_map = FeatureMap(params['dataset_id'], data_dir)\n    feature_map.load(feature_map_json, params)\n    logging.info(\"Feature specs: \" + print_to_json(feature_map.features))\n    model_class = getattr(model_zoo, params['model'])\n    model = model_class(feature_map, **params)\n    model.count_parameters()\n    params[\"data_loader\"] = MMCTRDataLoader\n    train_gen, valid_gen = RankDataLoader(feature_map, stage='train', **params).make_iterator()\n    model.load_weights(model.checkpoint)\n    logging.info('Test scoring...')\n    test_gen = RankDataLoader(feature_map, stage='test', **params).make_iterator()\n    test_pred = model.predict(test_gen)\n    ans = pd.DataFrame({\"ID\": range(test_pred.shape[0]), \"Task2\": test_pred})\n    logging.info(\"Writing results...\")\n    os.makedirs(\"submission\", exist_ok=True)\n    ans.to_csv(\"submission/prediction.csv\", index=False)\n    shutil.make_archive(f'submission/{experiment_id}', 'zip', 'submission/', 'prediction.csv')\n    logging.info(\"All done.\")\"\"\"\n\nfiles[\"src/DIN.py\"] = \"\"\"from fuxictr.pytorch.models import BaseModel\\nclass DIN(BaseModel): pass\"\"\"\nfiles[\"src/Transformer_DCN_Quant.py\"] = \"\"\"from fuxictr.pytorch.models import BaseModel\\nclass Transformer_DCN_Quant(BaseModel): pass\"\"\"\n\nfor path, content in files.items():\n    with open(path, \"w\") as f: f.write(content)\nprint(\"✓ Model source files created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T16:08:10.039315Z","iopub.execute_input":"2025-12-05T16:08:10.039618Z","iopub.status.idle":"2025-12-05T16:08:10.054525Z","shell.execute_reply.started":"2025-12-05T16:08:10.039594Z","shell.execute_reply":"2025-12-05T16:08:10.053795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Write Configuration (Optimized for Speed + New Embeddings)\nimport json\nimport os\n\n# Create directories just in case\nos.makedirs(\"config/Transformer_DCN_microlens_mmctr_tuner_config_01\", exist_ok=True)\nos.makedirs(\"data/MicroLens_1M_x1\", exist_ok=True)\n\n# DATASET CONFIG (Points to our new item_info_task1.parquet)\ndataset_config = \"\"\"MicroLens_1M_x1:\n    data_format: parquet\n    data_root: ./data/\n    feature_cols:\n    - {active: true, dtype: int, name: user_id, type: meta}\n    - {active: true, dtype: int, name: item_seq, type: meta}\n    - {active: true, dtype: int, name: likes_level, type: categorical, vocab_size: 11}\n    - {active: true, dtype: int, name: views_level, type: categorical, vocab_size: 11}\n    - {active: true, dtype: int, name: item_id, source: item, type: categorical, vocab_size: 91718}\n    - {active: true, dtype: int, max_len: 5, name: item_tags, source: item, type: sequence, vocab_size: 11740}\n    - {active: true, dtype: float, embedding_dim: 128, name: item_emb_d128, source: item, type: embedding}\n    item_info: ./data/MicroLens_1M_x1/item_info_task1.parquet\n    label_col: {dtype: float, name: label}\n    rebuild_dataset: False\n    test_data: ./data/MicroLens_1M_x1/test.parquet\n    train_data: ./data/MicroLens_1M_x1/train.parquet\n    valid_data: ./data/MicroLens_1M_x1/valid.parquet\"\"\"\n\nwith open(\"config/Transformer_DCN_microlens_mmctr_tuner_config_01/dataset_config.yaml\", \"w\") as f: \n    f.write(dataset_config)\n\n# MODEL CONFIG (Reduced Epochs/Dims for Kaggle Time Limits)\nmodel_config = \"\"\"Transformer_DCN_MicroLens_1M_x1_001_820c435c:\n    batch_norm: false\n    batch_size: 1024\n    concat_max_pool: true\n    dataset_id: MicroLens_1M_x1\n    dcn_cross_layers: 3\n    dcn_hidden_units: [512, 256]\n    debug_mode: false\n    dim_feedforward: 128\n    early_stop_patience: 2\n    embedding_dim: 32\n    embedding_regularizer: 0\n    epochs: 5\n    eval_steps: null\n    feature_config: null\n    feature_specs: null\n    first_k_cols: 8\n    group_id: user_id\n    hidden_activations: relu\n    learning_rate: 0.001\n    loss: binary_crossentropy\n    metrics: [logloss, AUC]\n    mlp_hidden_units: [32]\n    model: Transformer_DCN\n    model_root: ./checkpoints/\n    monitor: {AUC: 1}\n    monitor_mode: max\n    net_dropout: 0.1\n    net_regularizer: 0\n    num_heads: 1\n    num_workers: 8\n    optimizer: adam\n    pickle_feature_encoder: true\n    save_best_only: true\n    seed: 20242025\n    shuffle: true\n    task: binary_classification\n    transformer_dropout: 0.1\n    transformer_layers: 1\n    use_features: null\n    verbose: 1\"\"\"\n\nwith open(\"config/Transformer_DCN_microlens_mmctr_tuner_config_01/model_config.yaml\", \"w\") as f: \n    f.write(model_config)\n\n# MANUAL FEATURE MAP (Fixes schema errors)\nfeature_map = {\n    \"dataset_id\": \"MicroLens_1M_x1\", \"num_fields\": 7, \"num_features\": -1, \"input_length\": -1,\n    \"features\": {\n        \"user_id\": {\"active\": True, \"dtype\": \"int\", \"name\": \"user_id\", \"type\": \"meta\"},\n        \"item_seq\": {\"active\": True, \"dtype\": \"int\", \"name\": \"item_seq\", \"type\": \"meta\"},\n        \"likes_level\": {\"active\": True, \"dtype\": \"int\", \"name\": \"likes_level\", \"type\": \"categorical\", \"vocab_size\": 11},\n        \"views_level\": {\"active\": True, \"dtype\": \"int\", \"name\": \"views_level\", \"type\": \"categorical\", \"vocab_size\": 11},\n        \"item_id\": {\"active\": True, \"dtype\": \"int\", \"name\": \"item_id\", \"source\": \"item\", \"type\": \"categorical\", \"vocab_size\": 91718},\n        \"item_tags\": {\"active\": True, \"dtype\": \"int\", \"max_len\": 5, \"name\": \"item_tags\", \"source\": \"item\", \"type\": \"sequence\", \"vocab_size\": 11740},\n        \"item_emb_d128\": {\"active\": True, \"dtype\": \"float\", \"embedding_dim\": 128, \"name\": \"item_emb_d128\", \"source\": \"item\", \"type\": \"embedding\"}\n    },\n    \"labels\": [\"label\"]\n}\nwith open(\"data/MicroLens_1M_x1/feature_map.json\", \"w\") as f: \n    json.dump(feature_map, f, indent=4)\n\n# Link Data\nimport glob\ntrain_files = glob.glob(\"/kaggle/input/microlens/train.parquet\", recursive=True)\nif train_files:\n    dataset_dir = os.path.dirname(train_files[0])\n    local_data_dir = \"./data/MicroLens_1M_x1\"\n    for f in os.listdir(dataset_dir):\n        src = os.path.join(dataset_dir, f)\n        dst = os.path.join(local_data_dir, f)\n        if not os.path.exists(dst): os.symlink(src, dst)\n    print(\"✓ Data linked\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T16:10:52.494332Z","iopub.execute_input":"2025-12-05T16:10:52.495232Z","iopub.status.idle":"2025-12-05T16:10:52.518680Z","shell.execute_reply.started":"2025-12-05T16:10:52.495200Z","shell.execute_reply":"2025-12-05T16:10:52.518045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Run Training & Prediction\nprint(\"Starting Training...\")\n!python run_expid.py --config config/Transformer_DCN_microlens_mmctr_tuner_config_01 --expid Transformer_DCN_MicroLens_1M_x1_001_820c435c --gpu 0\n\nprint(\"Starting Prediction...\")\n!python prediction.py --config config/Transformer_DCN_microlens_mmctr_tuner_config_01 --expid Transformer_DCN_MicroLens_1M_x1_001_820c435c --gpu 0\n\nprint(\"✓ Done! Submission file is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T17:13:36.939590Z","iopub.execute_input":"2025-12-05T17:13:36.939867Z","iopub.status.idle":"2025-12-05T18:35:21.363706Z","shell.execute_reply.started":"2025-12-05T17:13:36.939837Z","shell.execute_reply":"2025-12-05T18:35:21.362758Z"}},"outputs":[{"name":"stdout","text":"Starting Training...\n2025-12-05 17:13:39,647 P526 INFO FuxiCTR version: 2.3.7\n2025-12-05 17:13:39,647 P526 INFO Params: {\n    \"batch_norm\": \"False\",\n    \"batch_size\": \"1024\",\n    \"concat_max_pool\": \"True\",\n    \"data_format\": \"parquet\",\n    \"data_root\": \"./data/\",\n    \"dataset_id\": \"MicroLens_1M_x1\",\n    \"dcn_cross_layers\": \"3\",\n    \"dcn_hidden_units\": \"[512, 256]\",\n    \"debug_mode\": \"False\",\n    \"dim_feedforward\": \"128\",\n    \"early_stop_patience\": \"2\",\n    \"embedding_dim\": \"32\",\n    \"embedding_regularizer\": \"0\",\n    \"epochs\": \"5\",\n    \"eval_steps\": \"None\",\n    \"feature_cols\": \"[{'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}, {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}, {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}]\",\n    \"feature_config\": \"None\",\n    \"feature_specs\": \"None\",\n    \"first_k_cols\": \"8\",\n    \"gpu\": \"0\",\n    \"group_id\": \"user_id\",\n    \"hidden_activations\": \"relu\",\n    \"item_info\": \"./data/MicroLens_1M_x1/item_info_task1.parquet\",\n    \"label_col\": \"{'dtype': 'float', 'name': 'label'}\",\n    \"learning_rate\": \"0.001\",\n    \"loss\": \"binary_crossentropy\",\n    \"metrics\": \"['logloss', 'AUC']\",\n    \"mlp_hidden_units\": \"[32]\",\n    \"model\": \"Transformer_DCN\",\n    \"model_id\": \"Transformer_DCN_MicroLens_1M_x1_001_820c435c\",\n    \"model_root\": \"./checkpoints/\",\n    \"monitor\": \"{'AUC': 1}\",\n    \"monitor_mode\": \"max\",\n    \"net_dropout\": \"0.1\",\n    \"net_regularizer\": \"0\",\n    \"num_heads\": \"1\",\n    \"num_workers\": \"8\",\n    \"optimizer\": \"adam\",\n    \"pickle_feature_encoder\": \"True\",\n    \"rebuild_dataset\": \"False\",\n    \"save_best_only\": \"True\",\n    \"seed\": \"20242025\",\n    \"shuffle\": \"True\",\n    \"task\": \"binary_classification\",\n    \"test_data\": \"./data/MicroLens_1M_x1/test.parquet\",\n    \"train_data\": \"./data/MicroLens_1M_x1/train.parquet\",\n    \"transformer_dropout\": \"0.1\",\n    \"transformer_layers\": \"1\",\n    \"use_features\": \"None\",\n    \"valid_data\": \"./data/MicroLens_1M_x1/valid.parquet\",\n    \"verbose\": \"1\"\n}\n2025-12-05 17:13:39,648 P526 INFO Set up feature processor...\n2025-12-05 17:13:39,648 P526 INFO Fit feature processor...\n2025-12-05 17:13:39,648 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}\n2025-12-05 17:13:39,648 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}\n2025-12-05 17:13:39,648 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-05 17:13:39,648 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-05 17:13:39,648 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}\n2025-12-05 17:13:39,673 P526 INFO Processing column: {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}\n2025-12-05 17:13:39,676 P526 INFO Processing column: {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}\n2025-12-05 17:13:39,676 P526 INFO Set column index...\n2025-12-05 17:13:39,676 P526 INFO Save feature_map to json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-05 17:13:39,676 P526 INFO Pickle feature_encode: ./data/MicroLens_1M_x1/feature_processor.pkl\n2025-12-05 17:13:39,681 P526 INFO Save feature_vocab to json: ./data/MicroLens_1M_x1/feature_vocab.json\n2025-12-05 17:13:39,824 P526 INFO Set feature processor done.\n2025-12-05 17:13:39,824 P526 INFO Load feature_map from json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-05 17:13:39,825 P526 INFO Set column index...\n2025-12-05 17:13:39,825 P526 INFO Feature specs: {\n    \"item_emb_d128\": \"{'source': 'item', 'type': 'embedding', 'embedding_dim': 128}\",\n    \"item_id\": \"{'source': 'item', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 91718}\",\n    \"item_seq\": \"{'type': 'meta'}\",\n    \"item_tags\": \"{'source': 'item', 'type': 'sequence', 'feature_encoder': 'layers.MaskedAveragePooling()', 'padding_idx': 0, 'max_len': 5, 'vocab_size': 11740}\",\n    \"likes_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\",\n    \"user_id\": \"{'type': 'meta'}\",\n    \"views_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\"\n}\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n  warnings.warn(\n2025-12-05 17:13:41,719 P526 INFO Total number of parameters: 47674689.\n2025-12-05 17:13:41,719 P526 INFO Loading datasets...\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-12-05 17:13:51,674 P526 INFO Train samples: total/3600000, blocks/1\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-12-05 17:13:52,036 P526 INFO Validation samples: total/10000, blocks/1\n2025-12-05 17:13:52,036 P526 INFO Loading train and validation data done.\n2025-12-05 17:13:52,037 P526 INFO Start training: 3516 batches/epoch\n2025-12-05 17:13:52,037 P526 INFO ************ Epoch=1 start ************\n100%|██████████████████████████████████████▉| 3515/3516 [16:07<00:00,  3.65it/s]2025-12-05 17:29:59,514 P526 INFO Train loss: 0.159004\n2025-12-05 17:29:59,514 P526 INFO Evaluation @epoch 1 - batch 3516: \n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 1/10 [00:00<00:07,  1.28it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:00<00:01,  3.68it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:01<00:00,  5.57it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:01<00:00,  7.05it/s]\u001b[A\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.95it/s]\u001b[A\n2025-12-05 17:30:01,209 P526 INFO [Metrics] AUC: 0.843107\n2025-12-05 17:30:01,209 P526 INFO Save best model: monitor(max)=0.843107\n100%|███████████████████████████████████████| 3516/3516 [16:09<00:00,  3.63it/s]\n2025-12-05 17:30:01,763 P526 INFO ************ Epoch=1 end ************\n  0%|                                                  | 0/3516 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n100%|██████████████████████████████████████▉| 3515/3516 [16:06<00:00,  3.67it/s]2025-12-05 17:46:08,364 P526 INFO Train loss: 0.058600\n2025-12-05 17:46:08,365 P526 INFO Evaluation @epoch 2 - batch 3516: \n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  3.36it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:01<00:00,  5.21it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:01<00:00,  6.69it/s]\u001b[A\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.59it/s]\u001b[A\n2025-12-05 17:46:10,164 P526 INFO [Metrics] AUC: 0.865891\n2025-12-05 17:46:10,165 P526 INFO Save best model: monitor(max)=0.865891\n100%|███████████████████████████████████████| 3516/3516 [16:08<00:00,  3.63it/s]\n2025-12-05 17:46:10,745 P526 INFO ************ Epoch=2 end ************\n  0%|                                                  | 0/3516 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n100%|██████████████████████████████████████▉| 3515/3516 [16:05<00:00,  3.66it/s]2025-12-05 18:02:16,094 P526 INFO Train loss: 0.041402\n2025-12-05 18:02:16,095 P526 INFO Evaluation @epoch 3 - batch 3516: \n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 1/10 [00:00<00:07,  1.23it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:01,  3.57it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:01<00:00,  5.44it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:01<00:00,  6.96it/s]\u001b[A\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.85it/s]\u001b[A\n2025-12-05 18:02:17,817 P526 INFO [Metrics] AUC: 0.850616\n2025-12-05 18:02:17,818 P526 INFO Monitor(max)=0.850616 STOP!\n2025-12-05 18:02:17,818 P526 INFO Reduce learning rate on plateau: 0.000100\n100%|███████████████████████████████████████| 3516/3516 [16:07<00:00,  3.64it/s]\n2025-12-05 18:02:17,915 P526 INFO ************ Epoch=3 end ************\n  0%|                                                  | 0/3516 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n100%|██████████████████████████████████████▉| 3515/3516 [16:01<00:00,  3.69it/s]2025-12-05 18:18:19,524 P526 INFO Train loss: 0.028824\n2025-12-05 18:18:19,524 P526 INFO Evaluation @epoch 4 - batch 3516: \n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 1/10 [00:00<00:07,  1.27it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.61it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:01<00:01,  4.98it/s]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:01<00:00,  6.71it/s]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:01<00:00,  7.95it/s]\u001b[A\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.90it/s]\u001b[A\n2025-12-05 18:18:21,230 P526 INFO [Metrics] AUC: 0.890809\n2025-12-05 18:18:21,231 P526 INFO Save best model: monitor(max)=0.890809\n100%|███████████████████████████████████████| 3516/3516 [16:03<00:00,  3.65it/s]\n2025-12-05 18:18:21,806 P526 INFO ************ Epoch=4 end ************\n  0%|                                                  | 0/3516 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n100%|██████████████████████████████████████▉| 3515/3516 [15:57<00:00,  3.67it/s]2025-12-05 18:34:19,589 P526 INFO Train loss: 0.026086\n2025-12-05 18:34:19,590 P526 INFO Evaluation @epoch 5 - batch 3516: \n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 1/10 [00:00<00:07,  1.24it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:00<00:01,  3.62it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:01<00:00,  5.51it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:01<00:00,  7.00it/s]\u001b[A\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  5.85it/s]\u001b[A\n2025-12-05 18:34:21,312 P526 INFO [Metrics] AUC: 0.892394\n2025-12-05 18:34:21,313 P526 INFO Save best model: monitor(max)=0.892394\n100%|███████████████████████████████████████| 3516/3516 [16:00<00:00,  3.66it/s]\n2025-12-05 18:34:21,898 P526 INFO ************ Epoch=5 end ************\n2025-12-05 18:34:21,898 P526 INFO Training finished.\n2025-12-05 18:34:21,898 P526 INFO Load best model: /kaggle/working/checkpoints/MicroLens_1M_x1/Transformer_DCN_MicroLens_1M_x1_001_820c435c.model\n2025-12-05 18:34:22,071 P526 INFO ****** Validation evaluation ******\n  0%|                                                    | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  6.08it/s]\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:2898: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n  warnings.warn(\n2025-12-05 18:34:23,735 P526 INFO [Metrics] logloss: 1.410005 - AUC: 0.892394\nStarting Prediction...\n2025-12-05 18:34:27,952 P928 INFO FuxiCTR version: 2.3.7\n2025-12-05 18:34:27,953 P928 INFO Params: {\n    \"batch_norm\": \"False\",\n    \"batch_size\": \"1024\",\n    \"concat_max_pool\": \"True\",\n    \"data_format\": \"parquet\",\n    \"data_root\": \"./data/\",\n    \"dataset_id\": \"MicroLens_1M_x1\",\n    \"dcn_cross_layers\": \"3\",\n    \"dcn_hidden_units\": \"[512, 256]\",\n    \"debug_mode\": \"False\",\n    \"dim_feedforward\": \"128\",\n    \"early_stop_patience\": \"2\",\n    \"embedding_dim\": \"32\",\n    \"embedding_regularizer\": \"0\",\n    \"epochs\": \"5\",\n    \"eval_steps\": \"None\",\n    \"feature_cols\": \"[{'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}, {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}, {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}, {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}, {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}]\",\n    \"feature_config\": \"None\",\n    \"feature_specs\": \"None\",\n    \"first_k_cols\": \"8\",\n    \"gpu\": \"0\",\n    \"group_id\": \"user_id\",\n    \"hidden_activations\": \"relu\",\n    \"item_info\": \"./data/MicroLens_1M_x1/item_info_task1.parquet\",\n    \"label_col\": \"{'dtype': 'float', 'name': 'label'}\",\n    \"learning_rate\": \"0.001\",\n    \"loss\": \"binary_crossentropy\",\n    \"metrics\": \"['logloss', 'AUC']\",\n    \"mlp_hidden_units\": \"[32]\",\n    \"model\": \"Transformer_DCN\",\n    \"model_id\": \"Transformer_DCN_MicroLens_1M_x1_001_820c435c\",\n    \"model_root\": \"./checkpoints/\",\n    \"monitor\": \"{'AUC': 1}\",\n    \"monitor_mode\": \"max\",\n    \"net_dropout\": \"0.1\",\n    \"net_regularizer\": \"0\",\n    \"num_heads\": \"1\",\n    \"num_workers\": \"8\",\n    \"optimizer\": \"adam\",\n    \"pickle_feature_encoder\": \"True\",\n    \"rebuild_dataset\": \"False\",\n    \"save_best_only\": \"True\",\n    \"seed\": \"20242025\",\n    \"shuffle\": \"True\",\n    \"task\": \"binary_classification\",\n    \"test_data\": \"./data/MicroLens_1M_x1/test.parquet\",\n    \"train_data\": \"./data/MicroLens_1M_x1/train.parquet\",\n    \"transformer_dropout\": \"0.1\",\n    \"transformer_layers\": \"1\",\n    \"use_features\": \"None\",\n    \"valid_data\": \"./data/MicroLens_1M_x1/valid.parquet\",\n    \"verbose\": \"1\"\n}\n2025-12-05 18:34:27,954 P928 INFO Set up feature processor...\n2025-12-05 18:34:27,954 P928 INFO Fit feature processor...\n2025-12-05 18:34:27,954 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'user_id', 'type': 'meta'}\n2025-12-05 18:34:27,954 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_seq', 'type': 'meta'}\n2025-12-05 18:34:27,954 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'likes_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-05 18:34:27,954 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'views_level', 'type': 'categorical', 'vocab_size': 11}\n2025-12-05 18:34:27,954 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'name': 'item_id', 'source': 'item', 'type': 'categorical', 'vocab_size': 91718}\n2025-12-05 18:34:27,980 P928 INFO Processing column: {'active': True, 'dtype': 'int', 'max_len': 5, 'name': 'item_tags', 'source': 'item', 'type': 'sequence', 'vocab_size': 11740}\n2025-12-05 18:34:27,982 P928 INFO Processing column: {'active': True, 'dtype': 'float', 'embedding_dim': 128, 'name': 'item_emb_d128', 'source': 'item', 'type': 'embedding'}\n2025-12-05 18:34:27,983 P928 INFO Set column index...\n2025-12-05 18:34:27,983 P928 INFO Save feature_map to json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-05 18:34:27,983 P928 INFO Pickle feature_encode: ./data/MicroLens_1M_x1/feature_processor.pkl\n2025-12-05 18:34:27,988 P928 INFO Save feature_vocab to json: ./data/MicroLens_1M_x1/feature_vocab.json\n2025-12-05 18:34:28,136 P928 INFO Set feature processor done.\n2025-12-05 18:34:28,136 P928 INFO Load feature_map from json: ./data/MicroLens_1M_x1/feature_map.json\n2025-12-05 18:34:28,136 P928 INFO Set column index...\n2025-12-05 18:34:28,137 P928 INFO Feature specs: {\n    \"item_emb_d128\": \"{'source': 'item', 'type': 'embedding', 'embedding_dim': 128}\",\n    \"item_id\": \"{'source': 'item', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 91718}\",\n    \"item_seq\": \"{'type': 'meta'}\",\n    \"item_tags\": \"{'source': 'item', 'type': 'sequence', 'feature_encoder': 'layers.MaskedAveragePooling()', 'padding_idx': 0, 'max_len': 5, 'vocab_size': 11740}\",\n    \"likes_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\",\n    \"user_id\": \"{'type': 'meta'}\",\n    \"views_level\": \"{'source': '', 'type': 'categorical', 'padding_idx': 0, 'vocab_size': 11}\"\n}\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n  warnings.warn(\n2025-12-05 18:34:30,085 P928 INFO Total number of parameters: 47674689.\n2025-12-05 18:34:30,085 P928 INFO Loading datasets...\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-12-05 18:34:40,032 P928 INFO Train samples: total/3600000, blocks/1\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-12-05 18:34:40,408 P928 INFO Validation samples: total/10000, blocks/1\n2025-12-05 18:34:40,408 P928 INFO Loading train and validation data done.\n2025-12-05 18:34:40,586 P928 INFO Test scoring...\n2025-12-05 18:34:40,587 P928 INFO Loading datasets...\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-12-05 18:34:42,315 P928 INFO Test samples: total/379142, blocks/1\n2025-12-05 18:34:42,315 P928 INFO Loading test data done.\n100%|█████████████████████████████████████████| 371/371 [00:36<00:00, 10.19it/s]\n2025-12-05 18:35:18,820 P928 INFO Writing results...\n2025-12-05 18:35:20,343 P928 INFO All done.\n✓ Done! Submission file is ready.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}